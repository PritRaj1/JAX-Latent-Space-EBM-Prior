# JAX-Latent-Space-EBM-Prior
A JAX implementation of the Learning Latent Space Energy-Based Prior Model, presented by [Pang et al. (2020)](https://proceedings.neurips.cc/paper_files/paper/2020/file/fa3060edb66e6ff4507886f9912e1ab9-Paper.pdf). Thermodynamic Integration, presented by [Calderhead and Girolami (2009)](https://www.sciencedirect.com/science/article/pii/S0167947309002722),
 has also been implemented as a means of exerting control over learning gradient variance.

The comprehensive **42-page** thesis has been uploaded [here](https://github.com/PritRaj1/JAX-ThermoEBM/blob/main/MEng%20Report.pdf). Everything is detailed in this document for your reference.

## Introduction

The performances of deep generative models depend on the distributional characteristics of their learning gradients. Despite this, the exact influence of learning gradient variance remains poorly understood, and investigations into the topic are bounded by our limited ability to control gradient variance. For example, managing gradient variance through batching alone is challenging, especially under constrained computational resources.

To address this, we propose leveraging Thermodynamic Integration as a means of robustly controlling the learning gradient variance. This is achieved by parameterising the temperature schedule used to evaluate the thermodynamic integral. This parameterisation allows us to exert precise control over the variances in estimates of latent space variables derived through Markov chain Monte Carlo (MCMC) sampling, as well as the error in Monte Carlo estimates of the mean. The method is subsequently proven and applied to investigate the relationship between learning gradient variance and the fidelity of images generated by the latent space energy-based prior model introduced by [Pang et al., 2020](https://proceedings.neurips.cc/paper_files/paper/2020/file/fa3060edb66e6ff4507886f9912e1ab9-Paper.pdf). 

This study reveals that although there is a notable relationship between learning gradient variance and image fidelity, learning gradient variance alone is inadequate as a predictor of the generative capacity of the latent space energy-based prior model. Instead, the study demonstrates that the temperature schedule itself exerts an even greater influence on image fidelity, serving as a direct reflection of the balance between exploration and exploitation that the deep generative model maintains over the loss landscape. 

## Does learning gradient variance matter?

Yes! Learning gradient variance, \( \mathrm{Var}_\theta\left[\nabla_\theta \mathcal{L}(\theta, \mathbf{x})\right] \) , does matter!

## To run

To get started, follow these steps:

1. Make sure you have Python version 3.9 or higher installed.
 
2. Install the required dependencies by running:

```bash
pip install -r requirements.txt
```

3. Edit the hyperparameters in the `hyperparams.ini` file according to your experiment setup.

4. Run the main experiment script `main.py` to gather CSV logs. You can do this by executing:

```bash
python main.py
```




